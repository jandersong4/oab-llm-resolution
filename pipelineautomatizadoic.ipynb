{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "def run_gemini(model_name, prompt):\n",
    "    print(\"O run_gemini est√° sendo usado!\")\n",
    "    print(f\"O prompt que est√° sendo usado √©: {prompt}!\")\n",
    "    generation_config = {\n",
    "    \"candidate_count\": 1,\n",
    "    }\n",
    "    safety_settings = {\n",
    "    \"HARASSMENT\": \"BLOCK_NONE\",\n",
    "    \"HATE\": \"BLOCK_NONE\",\n",
    "    \"SEXUAL\": \"BLOCK_NONE\",\n",
    "    \"DANGEROUS\": \"BLOCK_NONE\",\n",
    "    }\n",
    "    testeFinal = tokenAPI1\n",
    "    teste = tokenAPI2\n",
    "    mae = tokenAPI3\n",
    "    eu = tokenAPI4\n",
    "    pai1 = tokenAPI5\n",
    "    pai2 = tokenAPI6\n",
    "    jogos = tokenAPI7\n",
    "    listaAPIs = [testeFinal, teste, mae, eu, pai1, pai2, jogos]\n",
    "    nomesAPIs = [\"testeFinal\", \"teste\", \"mae\", \"eu\", \"pai1\", \"pai2\", \"jogos\"]\n",
    "    index = 0\n",
    "    nomeModelo = model_name.replace(\"google/\", \"models/\")\n",
    "    while len(listaAPIs) >= 1:\n",
    "        GOOGLE_API_KEY = listaAPIs[index]\n",
    "        nomeAPI = nomesAPIs[index]\n",
    "        print(f\"A API √©: {nomeAPI}\")\n",
    "        print()\n",
    "        print()\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=nomeModelo, generation_config=generation_config, safety_settings=safety_settings)\n",
    "        try:\n",
    "            resposta_completa = model.generate_content(prompt)\n",
    "            resposta = resposta_completa.text\n",
    "        except Exception as e:\n",
    "            print(\"Erro inesperado\")\n",
    "            listaAPIs.pop(0)\n",
    "            nomesAPIs.pop(0)\n",
    "            # time.sleep(20)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    if len(listaAPIs) == 0:\n",
    "        print(\"Nenhuma API restante\")\n",
    "        return False\n",
    "\n",
    "    l = list()\n",
    "    l.append(dict())\n",
    "    l[0]['generated_text'] = resposta\n",
    "\n",
    "    time.sleep(30)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=jsCUDeg_Op4&t=185s\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=tokenHuggingFace)\n",
    "print('Deu certo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "import shutil\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from accelerate import dispatch_model\n",
    "\n",
    "def load_model(model_name, quant):\n",
    "    # Enable quantization using bitsandbytes (e.g., 4-bit)\n",
    "    if quant:\n",
    "      quantization_config = BitsAndBytesConfig(\n",
    "          load_in_4bit=True,  # Enable 4-bit quantization\n",
    "          bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computations\n",
    "          bnb_4bit_use_double_quant=True,  # Use double quantization for memory efficiency\n",
    "          bnb_4bit_quant_type=\"nf4\"  # Use normalized float4 for better accuracy\n",
    "      )\n",
    "    else:\n",
    "      quantization_config = BitsAndBytesConfig(\n",
    "          load_in_4bit=False,  # Enable 4-bit quantization\n",
    "          bnb_4bit_compute_dtype=torch.float32,  # Use float16 for computations\n",
    "          bnb_4bit_use_double_quant=False,  # Use double quantization for memory efficiency\n",
    "          bnb_4bit_quant_type=\"nf4\"  # Use normalized float4 for better accuracy\n",
    "      )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load tokenizer and quantized model\n",
    "    if model_name == \"google/flan-t5-small\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",  # Automatically map the model to available devices\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"cuda\",  # Automatically map the model to available devices\n",
    "        )\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = pipeline(\n",
    "       \"text2text-generation\" if model == \"google/flan-t5-small\" else \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print('MODEL LOADED!', elapsed_time, \"secs\")\n",
    "    return pipe, model\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Retorna o uso da mem√≥ria da GPU.\"\"\"\n",
    "    return torch.cuda.memory_allocated() / (1024 ** 2)  # Em MB\n",
    "\n",
    "def get_disk_usage():\n",
    "    \"\"\"Retorna o uso do espa√ßo em disco do cache do Hugging Face.\"\"\"\n",
    "    cache_path = \"/root/.cache/huggingface\"\n",
    "    if os.path.exists(cache_path):\n",
    "        return shutil.disk_usage(cache_path).used / (1024 ** 2)  # Em MB\n",
    "    return 0\n",
    "\n",
    "def unload_model(model, pipe):\n",
    "    \"\"\"Libera mem√≥ria da GPU e do disco, e exibe os resultados.\"\"\"\n",
    "\n",
    "    print(\"\\nüìå **Antes da libera√ß√£o:**\")\n",
    "    gpu_before = get_gpu_memory()\n",
    "    disk_before = get_disk_usage()\n",
    "    print(f\"üîπ Mem√≥ria da GPU usada: {gpu_before:.2f} MB\")\n",
    "    print(f\"üîπ Espa√ßo usado no cache do Hugging Face: {disk_before:.2f} MB\")\n",
    "\n",
    "    # üîΩ Libera a GPU\n",
    "    if model is not None:\n",
    "        model.to(\"cpu\")  # Move para CPU\n",
    "        del model  # Remove refer√™ncia ao modelo\n",
    "\n",
    "    if pipe is not None:\n",
    "        del pipe  # Remove refer√™ncia ao pipeline\n",
    "\n",
    "    gc.collect()  # For√ßa a coleta de lixo\n",
    "    torch.cuda.empty_cache()  # Limpa a VRAM\n",
    "\n",
    "    print(\"\\n‚úÖ **Ap√≥s limpar a GPU:**\")\n",
    "    gpu_after = get_gpu_memory()\n",
    "    print(f\"üîπ Mem√≥ria da GPU usada: {gpu_after:.2f} MB (Deveria diminuir!)\")\n",
    "\n",
    "    # üîΩ Remove cache do Hugging Face\n",
    "    cache_path = \"/root/.cache/huggingface\"\n",
    "    if os.path.exists(cache_path):\n",
    "        shutil.rmtree(cache_path)\n",
    "\n",
    "    print(\"\\n‚úÖ **Ap√≥s remover o cache do Hugging Face:**\")\n",
    "    disk_after = get_disk_usage()\n",
    "    print(f\"üîπ Espa√ßo usado no cache do Hugging Face: {disk_after:.2f} MB (Deveria ser 0!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def small_test(pipe):\n",
    "    # Example usage\n",
    "    prompt = \"What is 2 + 2?\"\n",
    "    result = pipe(prompt, max_length=200, num_return_sequences=1)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_prompt(question, dica):\n",
    "    prompt = \"\"\n",
    "    answer_separator = \".\"\n",
    "    prompt = \"Responda √† seguinte quest√£o de m√∫ltipla escolha no formato 'Alternativa: LETRA, justificativa'. Com LETRA devendo ser substitu√≠da pela letra da alternativa que voc√™ acha correta ou a mais correta dentre as alternativas da quest√£o. Caso n√£o considere nenhuma certa, substitua LETRA por X e justificativa devendo ser substitu√≠da pelo racioc√≠nio que te levou a escolher a LETRA.\\n\"\n",
    "    if dica:\n",
    "        prompt += f'''Utilize exclusivamente o ferramental juridico fornecido para respond√™-la. N√£o √∫tilize outras leis al√©m das fornecidas.\\n\n",
    "                     -Ferramental Jur√≠dico:\n",
    "                     {question[legalTools]}\\n'''\n",
    "    prompt += \"Segue a quest√£o:\\n\"\n",
    "    prompt += \"Quest√£o: \\n\" + question[\"question\"] + \"\\n\"\n",
    "    return prompt\n",
    "    \n",
    "    print('Prompt built.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def map_multiple_choice(text):\n",
    "    \"\"\"\n",
    "    Parses a text to map multiple-choice letters to their answers and finds the letter inside the boxed notation.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text containing multiple-choice options and a boxed answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping letters to answers.\n",
    "        str: The letter corresponding to the boxed answer.\n",
    "    \"\"\"\n",
    "    # Regular expression to extract options like \"A. 1\", \"B. 3\", etc.\n",
    "    options = re.findall(r\"([A-E])\\.\\s*([\\d\\.]+)\", text)\n",
    "    \n",
    "    # Create a mapping of letter -> answer\n",
    "    # letter_to_answer = {letter: float(answer) for letter, answer in options}\n",
    "    letter_to_answer = {}\n",
    "    for letter, answer in options:\n",
    "        try:\n",
    "            letter_to_answer[letter] = float(answer)\n",
    "        except Exception as e:\n",
    "            print(\"N√£o foi poss√≠vel fazer o mapeamento!!\")\n",
    "            return \"X\"\n",
    "\n",
    "    # Extract the boxed answer using \\boxed{}\n",
    "    boxed_match = re.search(r\"\\\\boxed\\{([\\d\\.]+)\\}\", text)\n",
    "    if boxed_match:\n",
    "        boxed_answer = float(boxed_match.group(1))\n",
    "    else:\n",
    "        return 'X'\n",
    "        \n",
    "    # Find the letter corresponding to the boxed answer\n",
    "    boxed_letter = None\n",
    "    for letter, answer in letter_to_answer.items():\n",
    "        if answer == boxed_answer:\n",
    "            boxed_letter = letter\n",
    "            break\n",
    "    if boxed_letter is None:\n",
    "        return 'X'\n",
    "    return boxed_letter\n",
    "\n",
    "def parse_answer(full_answer):\n",
    "    listaPadroes = \"\"\n",
    "    padroesPortugues = [\n",
    "        r\"Alternativa:\\s+([a-dA-D])\",\n",
    "        r\"\\*?\\*?Resposta\\*?\\*?:\\*?\\*?\\s*\\*?\\*?([a-d])\\*?\\*?\",\n",
    "        r\"\\*?Resposta:\\*?\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\*?resposta\\s*\\*?√©\\s*\\*?:\\*?\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\*?a\\s*\\*?op√ß√£o\\s*\\*?correta\\s*\\*?√©\\s*\\*?:\\*?\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\\\boxed\\{(?:\\\\text\\{(\\*?[a-d]\\*?)\\}|(\\*?[a-d]\\*?)[^a-d]*)\\}\",\n",
    "        r\"resposta √©\\s*\\*?([a-d])\\*?\",\n",
    "        r\"a op√ß√£o correta √© a\\s*\\*?([a-d])\\*?\",\n",
    "        r\"a op√ß√£o correta √©\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\\\boxed\\{([a-d])\\}\",\n",
    "        r\"resposta\\s*:\\s*\\[([a-d])\\]\",\n",
    "        r\"op√ß√£o correta\\s+√©\\s+(?:a\\s+)?(?:op√ß√£o\\s+)?([a-d])\",\n",
    "        r\"resposta correta\\s+√©\\s+(?:a\\s+)?([a-d])\",\n",
    "    ]\n",
    "    padroesIngles = [\n",
    "        r\"Alternativa:\\s+([a-dA-D])\",\n",
    "        r\"\\*?\\*?Answer\\*?\\*?:\\*?\\*?\\s*\\*?\\*?([a-d])\\*?\\*?\",\n",
    "        r\"\\*?Answer:\\*?\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\*?answer\\s*\\*?is\\s*\\*?:\\*?\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\*?the\\s*\\*?correct\\s*\\*?option\\s*\\*?is\\s*\\*?:\\*?\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\\\boxed\\{(?:\\\\text\\{(\\*?[a-d]\\*?)\\}|(\\*?[a-d]\\*?)[^a-d]*)\\}\",\n",
    "        r\"answer is\\s*\\*?([a-d])\\*?\",\n",
    "        r\"the correct option is\\s*\\*?([a-d])\\*?\",\n",
    "        r\"\\\\boxed\\{([a-d])\\}\",\n",
    "        r\"answer\\s*=\\s*([a-d])\",\n",
    "        r\"(?:Therefore, the correct answer is\\s*\\**([a-d])\\.*\\**)\",\n",
    "        r\"(?:Therefore, the correct answer is:\\s*\\n\\**([a-d])\\.*\\**)\",\n",
    "    ]\n",
    "    listaPadroes = [padroesPortugues, padroesIngles]\n",
    "    for tipoPadrao in listaPadroes:\n",
    "        for padrao in tipoPadrao:\n",
    "            match = re.search(padrao, full_answer, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1)  # Retorna a primeira correspond√™ncia encontrada\n",
    "    \n",
    "    return map_multiple_choice(full_answer)  # Retorna None se nenhum padr√£o for encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the directory and the archive name\n",
    "directory = '/kaggle/working'\n",
    "\n",
    "def zip_answers():\n",
    "    output_zip = f'answers.zip'\n",
    "    # Create the zip archive\n",
    "    with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
    "        for root, _, files in os.walk(directory):\n",
    "            num_files = len(files)\n",
    "            for file in files:\n",
    "                if file.startswith('answers_'):  # Filter files starting with \"answers_\"\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Add the file to the zip archive, maintaining its directory structure\n",
    "                    arcname = os.path.relpath(file_path, directory)\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        print(f\"Zip archive {output_zip} created successfully!\")\n",
    "        return output_zip\n",
    "\n",
    "\n",
    "    from IPython.display import FileLink\n",
    "    return FileLink(output_zip)\n",
    "\n",
    "zip_answers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "def is_equal(n1, n2):\n",
    "    # Convert n1 and n2 to Fraction if they are strings representing rational numbers\n",
    "    if n1 == n2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_last_digit(text):\n",
    "    match = re.search(r'-(\\d+)$', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.27.1/pipeline_tutorial#using-pipelines-on-a-dataset\n",
    "def prompts(questions, dica):\n",
    "    print(\"O prompts est√° sendo usado!\")\n",
    "    for question in questions:\n",
    "       prompt = build_prompt(question, dica)\n",
    "       yield prompt\n",
    "\n",
    "def call_gemini(questions, model_name, dica):\n",
    "    print(\"O call_gemini est√° sendo usado!\")\n",
    "    for question in questions:\n",
    "       prompt = build_prompt(question, dica)\n",
    "       yield run_gemini(model_name, prompt)\n",
    "\n",
    "def do_exam(questions_df, model_name, quant, pipe, temperature, prova, dica):\n",
    "    tipo_arquivo = \"\"\n",
    "    if dica:\n",
    "        tipo_arquivo = \"COM DICAS\"\n",
    "    else:\n",
    "        tipo_arquivo = \"SEM DICAS\"\n",
    "    total_time = 0\n",
    "    questions = questions_df.to_dict(orient='records')\n",
    "    print(f\"A prova √©: {prova}\")\n",
    "\n",
    "    question_numbers = list()\n",
    "    question_prompts = list()\n",
    "    question_bodies = list()\n",
    "\n",
    "    question_labels = list()\n",
    "    question_math_levels = list()\n",
    "    question_interpretation_levels = list()\n",
    "    question_types = list()\n",
    "    question_groups = list()\n",
    "\n",
    "    languages = list()\n",
    "    \n",
    "    llm_full_answers = list()\n",
    "    token_size_questions = list()\n",
    "    token_size_responses = list()\n",
    "\n",
    "    correct_answers = list()\n",
    "    is_correct_list = list()\n",
    "    times = list()\n",
    "    llm_letters = list()\n",
    "\n",
    "    prompt_types = list()\n",
    "\n",
    "    temperatures = list()\n",
    "\n",
    "    question_seq = -1\n",
    "    start_time = time.time()\n",
    "\n",
    "    if model_name == 'google/gemini-1.5-flash' or  \\\n",
    "            model_name == 'google/gemini-1.5-flash-8b' or \\\n",
    "            model_name == \"google/gemini-2.0-flash-exp\" or \\\n",
    "            model_name == \"google/gemini-1.5-pro\" or \\\n",
    "            model_name == \"google/gemini-1.0-pro\":\n",
    "        l = call_gemini(questions, model_name, dica)\n",
    "    else:\n",
    "        l = pipe(prompts(questions, dica), \n",
    "                 truncation=True, max_length = 1000, \n",
    "                 num_return_sequences=1)\n",
    "\n",
    "    last_question_s = 0\n",
    "    continuar = True\n",
    "    nome_arq = ''\n",
    "    for output in l:\n",
    "\n",
    "       if output is False:  # Se run_gemini retornar False, salvar os dados e sair\n",
    "            print(\"Erro detectado! Salvando as respostas j√° obtidas...\")\n",
    "            if dica:\n",
    "                df = pd.DataFrame({\n",
    "                    'question_number': questions_df['N√∫mero'][:(question_seq + 1)],  \n",
    "                    'question_body': questions_df['question'][:(question_seq + 1)],\n",
    "                    'full_model_name': model_name,\n",
    "                    'llm_chosen_answer': llm_letters,\n",
    "                    'correct_answer': correct_answers,\n",
    "                    'is_correct': is_correct_list,\n",
    "                    'legalTools':questions_df[\"legalTools\"][:(question_seq + 1)],\n",
    "                    'Anulada?': questions_df['Anulada?'][:(question_seq + 1)],\n",
    "                    'question_prompt': question_prompts,\n",
    "                    'response': llm_full_answers,\n",
    "                    'token_size_question': token_size_questions,\n",
    "                    'token_size_response': token_size_responses,\n",
    "                    'elapsed_time_sec': times\n",
    "                })\n",
    "            model_name = model_name.replace(\"/\", \".\")\n",
    "            nome_arq = prova + f\" {tipo_arquivo} - Modelo: \" + model_name \n",
    "            df.to_csv(f'{nome_arq}_at√©Quest√£o{question_seq}.csv')\n",
    "            continuar = False\n",
    "            return total_time, is_correct_list.count(True), continuar        \n",
    "       question_seq += 1\n",
    "       question = questions[question_seq]\n",
    "\n",
    "       prompt = build_prompt(question, dica)\n",
    "       full_answer = output[0]['generated_text']\n",
    "\n",
    "       print(\"\\n\\n############################################################\\n\\n\\n\")\n",
    "       print(\"A RESPOSTA COMPLETA FOI: \")\n",
    "       print(full_answer)\n",
    "       print(\"\\n\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\\n\\n\")\n",
    "       chosen_letter = parse_answer((full_answer.replace(prompt, \"\"))) #Tira trechos do prompt da resposta\n",
    "       if chosen_letter:\n",
    "           for char in chosen_letter:\n",
    "               if char.isalpha() and char.isupper():\n",
    "                   chosen_letter = char\n",
    "       else:\n",
    "           chosen_letter = \"X\"\n",
    "       end_time = time.time()\n",
    "       elapsed_time = end_time - start_time\n",
    "       start_time = time.time()\n",
    "\n",
    "       question_prompts.append(prompt)\n",
    "\n",
    "       token_size_questions.append(len(question['question'].split()))\n",
    "       token_size_responses.append(len(full_answer.split()))\n",
    "\n",
    "       temperatures.append(temperature)\n",
    "       llm_full_answers.append(full_answer)\n",
    "       llm_letters.append(chosen_letter)\n",
    "       correct_answers.append(question['answers'])\n",
    "       times.append(elapsed_time)\n",
    "\n",
    "       is_correct = is_equal(question['answers'], chosen_letter)\n",
    "       is_correct_list.append(is_correct)\n",
    "\n",
    "       total_time += elapsed_time\n",
    "\n",
    "       print(f\"{question['N√∫mero']} / {len(questions)}\",\n",
    "      is_correct,\n",
    "      \"O tempo gasto foi:\",\n",
    "      elapsed_time, \"chosen\", chosen_letter, \"correct\", question[\"answers\"], \"\\n\")\n",
    "\n",
    "    if dica:\n",
    "        df = pd.DataFrame({'question_number': questions_df['N√∫mero'],\n",
    "                           'question_body': questions_df['question'],\n",
    "                           'full_model_name': model_name,\n",
    "                           'model_family': model_name.split(\"/\")[0],\n",
    "                           'model_name': model_name.split(\"/\")[1],\n",
    "                           'llm_chosen_answer': llm_letters,\n",
    "                           'correct_answer': correct_answers,\n",
    "                           'is_correct': is_correct_list,\n",
    "                           'legalTools':questions_df[\"legalTools\"],\n",
    "                           'Anulada?': questions_df['Anulada?'],\n",
    "                             'question_prompt': question_prompts,\n",
    "                             'response': llm_full_answers,\n",
    "                             'token_size_question': token_size_questions,\n",
    "                             'token_size_response': token_size_responses,\n",
    "                             'elapsed_time_sec': times})\n",
    "    else:\n",
    "        df = pd.DataFrame({'question_number': questions_df['N√∫mero'],\n",
    "                           'question_body': questions_df['question'],\n",
    "                           'full_model_name': model_name,\n",
    "                           'model_family': model_name.split(\"/\")[0],\n",
    "                           'model_name': model_name.split(\"/\")[1],\n",
    "                           'llm_chosen_answer': llm_letters,\n",
    "                           'correct_answer': correct_answers,\n",
    "                           'is_correct': is_correct_list,\n",
    "                           'Anulada?': questions_df['Anulada?'],\n",
    "                             'question_prompt': question_prompts,\n",
    "                             'response': llm_full_answers,\n",
    "                             'token_size_question': token_size_questions,\n",
    "                             'token_size_response': token_size_responses,\n",
    "                             'elapsed_time_sec': times})\n",
    "    model_name = model_name.replace(\"/\", \".\")\n",
    "    nome_arq = prova + f\" {tipo_arquivo} - Modelo: \" + model_name + \" - \"\n",
    "    df.to_csv(f'{nome_arq}answers.csv')\n",
    "    return total_time, is_correct_list.count(True), continuar\n",
    "\n",
    "\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pequeno_teste_unload():\n",
    "    # Teste para verificar a fun√ß√£o unload_model\n",
    "    \n",
    "    model_name = \"distilgpt2\"  # Modelo pequeno para teste\n",
    "    \n",
    "    # Carrega o modelo e o tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")  # Carrega na GPU\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    \n",
    "    # Gera um texto curto para testar se o modelo funciona\n",
    "    output = pipe(\"Ol√°, mundo!\", max_length=20)\n",
    "    print(output)\n",
    "    \n",
    "    # Chama a fun√ß√£o para descarregar o modelo e liberar a mem√≥ria\n",
    "    unload_model(model, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def adicionandoColunas(df, dica):\n",
    "    df_novo = df.copy()\n",
    "    df_novo[\"N√∫mero\"] = df_novo.index + 1\n",
    "    df_novo[\"Anulada?\"] = df_novo[\"answers\"] == \"*\"\n",
    "    if dica:\n",
    "        df_novo = df_novo[[\"N√∫mero\", \"question\", \"comment\", \"legalTools\", \"answers\", \"Anulada?\"]]\n",
    "    else:\n",
    "        df_novo = df_novo[[\"N√∫mero\", \"question\", \"comment\", \"answers\", \"Anulada?\"]]\n",
    "    return df_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "modelos = [\n",
    "    # GEMINI\n",
    "    \"google/gemini-2.0-flash-exp\",\n",
    "    \"google/gemini-1.5-pro\",\n",
    "    \"google/gemini-1.5-flash\",\n",
    "    \"google/gemini-1.5-flash-8b\",\n",
    "    \"google/gemini-1.0-pro\"\n",
    "    \n",
    "    # DEEPSEEK\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", \n",
    "    \"deepseek-ai/deepseek-math-7b-instruct\", \n",
    "    \n",
    "   \n",
    "    # QWEN\n",
    "   \"Qwen/Qwen2.5-1.5B\", # small # 5 # feito 38, 39, 40, 41, 42\n",
    "   \"Qwen/Qwen2.5-7B\", # 6 # feito 38, 39, 40, 41, 42\n",
    "    # \"Qwen/Qwen2-Math-7B-Instruct\", # erro \"greed decoding\"\n",
    "   \"Qwen/Qwen2.5-3B-Instruct\", # 7 # feito 38, 39, 40, 41, 42\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\", # 8 # feito 38, 39, 40, 41, 42\n",
    "    \"Qwen/Qwen2.5-Math-7B-Instruct\", # 9 # feito 38, 39, 40, 41, 42\n",
    "   \"Qwen/Qwen2.5-Math-1.5B-Instruct\", # 10 # feito 38, 39, 40, 41, 42\n",
    "   \"Qwen/Qwen2.5-Math-7B-PRM800K\", # 11 # feito 38, 39, 40, 41, 42\n",
    "   \"Qwen/Qwen2.5-14B\", # 12 # feito 38, 39, 40, 41, 42\n",
    "   \"Qwen/Qwen2.5-14B-Instruct\", # 13 # feito 38, 39, 40, 41, 42\n",
    "   \"Qwen/Qwen2.5-Math-PRM-7B\", # 14 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "    # Vicuna\n",
    "    'lmsys/vicuna-7b-v1.5', # 15 # feito 38, 39, 40, 41, 42\n",
    "   'lmsys/vicuna-13b-v1.5', # 16 feito 38, 39, 40, 41, 42\n",
    "    \n",
    "    # Bloom\n",
    "    'bigscience/bloom-1b7', # 17 # feito 38, 39, 40, 41, 42\n",
    "    # 'bigscience/bloom-7b1' # erro de caminho incorreto\n",
    "\n",
    "    # LLEMMA\n",
    "   \"EleutherAI/llemma_7b\", # 18 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "    # Falcon\n",
    "   'tiiuae/falcon-7b-instruct', # 19 # feito 38, 39, 40, 41, 42\n",
    "   'tiiuae/falcon-7b', # 20 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "    # Falcon2\n",
    "    'tiiuae/falcon-11B', # 21 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "    # Falcon3\n",
    "    'tiiuae/Falcon3-3B-Instruct', # 22 # feito 38, 39, 40, 41, 42\n",
    "    'tiiuae/Falcon3-7B-Instruct', # 23 # feito 38, 39, 40, 41, 42\n",
    "    'tiiuae/Falcon3-7B-Base', # 24 # feito 38, 39, 40, 41, 42\n",
    "    'tiiuae/Falcon3-10B-Instruct', # 25 # feito 38, 39, 40, 41, 42\n",
    "    'tiiuae/Falcon3-10B-Base', # 26 # Feito 38, 39, 40, 41, 42\n",
    "    \n",
    "     # PHI\n",
    "   \"microsoft/phi-1\", # 27 # feito 38, 39, 40, 41, 42\n",
    "   \"microsoft/phi-1_5\", # 28 # feito 38, 39, 40, 41, 42\n",
    "   \"microsoft/phi-2\", # 29 # feito 38, 39, 40, 41, 42\n",
    "   \"microsoft/Phi-3-mini-4k-instruct\", # 30 #feito 38, 39, 40, 41, 42\n",
    "   \"microsoft/Phi-3-medium-128k-instruct\", # 31 #feito 38, 39, 40, 41, 42\n",
    "   \"microsoft/Phi-3.5-mini-instruct\", # 32 # feito 38, 39, 40, 41, 42 \n",
    "    # \"microsoft/Phi-3.5-MoE-instruct\", # N√ÉO FOI FEITOOOOOOOOOOO DEU SEM ESPA√áO\n",
    "   \"microsoft/phi-4\", # 33 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "     # NuminaMath\n",
    "     \"AI-MO/NuminaMath-7B-CoT\", # 34 # feito 38, 39, 40, 41, 42\n",
    "     # \"AI-MO/NuminaMath-7B-TIR-GPTQ\", ERRO CONFIG BITSANDBYTES\n",
    "     \"AI-MO/NuminaMath-7B-TIR\", # 35 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "     # Orca\n",
    "     \"microsoft/Orca-2-7b\", # 36 # feito 38, 39, 40, 41, 42\n",
    "     \"microsoft/Orca-2-13b\", # 37 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "     # Yi\n",
    "    \"01-ai/Yi-6B\", # 38 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "     # Mistral\n",
    "    # 'mistralai/Mathstral-7b-v0.1', # ACESSO RESTRITO\n",
    "    # \"mistralai/Mistral-7B-v0.1\", # ACESSO RESTRITO\n",
    "    # \"mistralai/Mistral-7B-v0.3\", # ACESSO RESTRITO\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.1\", # ACESSO RESTRITO\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\", # ACESSO RESTRITO\n",
    "    # \"mistralai/Mistral-Nemo-Instruct-2407\", # ACESSO RESTRITO\n",
    "    #  \"mistralai/Mixtral-8x7B-Instruct-v0.1\", # ACESSO RESTRITO\n",
    "     \"meta-math/MetaMath-Mistral-7B\", # 39 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "     # Sabia\n",
    "    \"maritaca-ai/sabia-7b\", # 40 # feito 38, 39, 40, 41, 42\n",
    "\n",
    "    # LLmama\n",
    "     # \"meta-llama/Meta-Llama-3-70B\", # ACESSO RESTRITO\n",
    "   # \"meta-llama/Meta-Llama-3-8B-Instruct\", # ACESSO RESTRITO\n",
    "    # \"meta-llama/Llama-2-7b\", # ACESSO RESTRITO\n",
    "   # \"meta-llama/Meta-Llama-3-8B\", # ACESSO RESTRITO\n",
    "\n",
    "    # Flan\n",
    "   # \"google/flan-t5-xxl\", # CONFIGURA√á√ÉO DESCONHECIDA PARA AUTOMODELCAUSALLM \n",
    "   \"google/flan-t5-small\", # 41  # feito 38, 39, 40, 41, 42\n",
    "    # \"google/mt5-base\", # CONFIGURA√á√ÉO DESCONHECIDA PARA AUTOMODELCAUSALLM\n",
    "    \n",
    "    # \"google/gemma-2-2b\", # ACESSO RESTRITO\n",
    "    # \"google/gemma-2-2b-it\", # ACESSO RESTRITO\n",
    "    #\"google/gemma-2-9b\", # ACESSO RESTRITO \n",
    "     # \"google/gemma-2-9b-it\", # ACESSO RESTRITO \n",
    "    #\"google/gemma-2-27b\", # ACESSO RESTRITO\n",
    "    ]\n",
    "\n",
    "print(\"LET'S GO!\", len(modelos), \" models.\")\n",
    "provas = {\"oab38\": [5], \"oab39\" : [], \"oab40\": [], \"oab41\": [], \"oab42\": []}\n",
    "diretorio = \"provas-oab-comentadas\"\n",
    "dica = False\n",
    "caminho_original = \"/kaggle/input/\"\n",
    "for prova, listaModelosUsados in provas.items():\n",
    "    print(f\"A prova √©: {prova}\")\n",
    "    caminho_diretorio = os.path.join(caminho_original, diretorio)\n",
    "    for arquivo in os.listdir(caminho_diretorio):\n",
    "        if prova in arquivo:\n",
    "            caminho_completo = os.path.join(caminho_diretorio, arquivo)\n",
    "        else:\n",
    "            print(\"ERROOOOOOOOOOO\")\n",
    "    print(f\"O caminho completo √©: {caminho_completo}\")\n",
    "    oab_exam_df = pd.read_csv(caminho_completo)\n",
    "    oab_exam_df = adicionandoColunas(oab_exam_df, dica)\n",
    "    for numero_modelo in listaModelosUsados:\n",
    "        model = \"\"\n",
    "        model_name = modelos[numero_modelo]\n",
    "        print(f\"Running {model_name}...\")\n",
    "        if model_name == 'google/gemini-1.5-flash' or  \\\n",
    "            model_name == 'google/gemini-1.5-flash-8b' or \\\n",
    "            model_name == \"google/gemini-2.0-flash-exp\" or \\\n",
    "            model_name == \"google/gemini-1.5-pro\" or \\\n",
    "            model_name == \"google/gemini-1.0-pro\":\n",
    "            pipe = None\n",
    "        else:\n",
    "            pipe, model = load_model(model_name, True)\n",
    "        dic = defaultdict()\n",
    "        dic[0.7] = defaultdict(float)\n",
    "        print(f\"Answering questions using {model_name}, temperature {0.7} and quant={True}...\")\n",
    "        total_time, ctt_score, continuar = do_exam(oab_exam_df, model_name, True, pipe, 0.7, prova, dica)\n",
    "        if continuar == False:\n",
    "          print(\"Deu ruim acabou o funcionamento!!!!\")\n",
    "          break\n",
    "        print(f\"Finished {model_name} after {total_time/60} minutes. Score: {ctt_score}\")\n",
    "        if pipe != None:\n",
    "            unload_model(model, pipe)\n",
    "        print(\"###########\", model_name)\n",
    "print('FINISHED!')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6983811,
     "sourceId": 11187453,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7185159,
     "sourceId": 11465860,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
